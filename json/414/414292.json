{"code": 0, "data": [{"cid": 2136493, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 414292, "reply_to": -1, "tag": null, "text": "æ€ä¹ˆçœ‹", "timestamp": 1627280420, "type": "text", "url": ""}, {"cid": 2136495, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 414292, "reply_to": -1, "tag": null, "text": "\n```\nprint('hello world')\n```\nå–„ç”¨markdown", "timestamp": 1627280430, "type": "text", "url": ""}, {"cid": 2136543, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 414292, "reply_to": -1, "tag": null, "text": "å…ˆå­¦å­¦æ€ä¹ˆåœ¨æ ‘æ´å‘ä»£ç å§ ä¹±æˆè¿™ä¸ªæ ·å­è°ç»™ä½ çœ‹\n```py\nimport torch\na = torch.tensor([1, 2, 3])\n```", "timestamp": 1627280799, "type": "text", "url": ""}, {"cid": 2136913, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "æ´ä¸»", "pid": 414292, "reply_to": -1, "tag": null, "text": "å•Šè¿™ æ‰‹æœºç«¯ä¸æ˜¾ç¤ºmarkdownğŸ˜­dzæ˜¯å¤šå±ååŒåœ¨ç”µè„‘ä¸Šç”¨çš„æ‰‹æœºappå‘çš„,è¿˜ä»¥ä¸ºæ²¡æ³•ç”¨ğŸ˜­", "timestamp": 1627284135, "type": "text", "url": ""}], "post": {"deleted": false, "image_metadata": {}, "likenum": 3, "pid": 414292, "reply": 4, "tag": null, "text": "æœ‰æ— å¥½å¿ƒäººå¸®å¿™çœ‹çœ‹lstmè¿‡æ‹ŸåˆTAT\nVersion:1.0 StartHTML:0000000128 EndHTML:0000016400 StartFragment:0000000128 EndFragment:0000016400 SourceURL:about:blank import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchtext.legacy import data\nfrom torchtext.legacy import datasets\nfrom torchtext.vocab import Vectors\n\nTEXT = data.Field()\nLABEL = data.Field(sequential=False, dtype=torch.long)\n\n\ntrain, val, test = datasets.SST.splits(\nTEXT, LABEL, fine_grained=True, train_subtrees=False)\n\n\n\nTEXT.build_vocab(train, vectors=Vectors(name='vector.txt', cache='./data'))\nLABEL.build_vocab(train)\n\n\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n(train, val, test), batch_size=128)\n\npretrained_embeddings = TEXT.vocab.vectors\ntorch.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\n\n\nclass RNN2(nn.Module):\ndef __init__(self):\nsuper(RNN2, self).__init__()\nself.embedding = nn.Embedding(len(TEXT.vocab), 300)\nself.embedding.from_pretrained(pretrained_embeddings)\nself.lstm = nn.LSTM(input_size=300, hidden_size=128, num_layers=2, dropout=0.5, bidirectional=True)\nself.LE = nn.Sequential(nn.Dropout(0.5), nn.Linear(128 * 4, 5))\n\ndef forward(self, x):\nx = self.embedding(x)\nx, out = self.lstm(x)\n\nf = self.LE(torch.cat((x[0], x[-1]), 1))\nreturn f\n\n\ndef train1(model, max_epochs, learning_rate, weight_decay=0.0):\nif torch.cuda.is_available():\nmodel.cuda()\ncost = torch.nn.CrossEntropyLoss()\noptimzer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nlosslist = []\nacclist = []\nfor epoch in range(max_epochs):\nrunning_loss = 0.0\nv_loss = 0.0\nrunning_correct = 0\nprint(\"Epoch{}/{}\".format(epoch, max_epochs))\nprint(\"-\" * 10)\n\nfor data1 in train_iter:\noptimzer.zero_grad()\nmodel.train()\nX_train, y_train = data1.text, (data1.label - 1)\n# a.append(X_train)\n# b.append(y_train)\nX_train, y_train = X_train.cuda(), y_train.cuda()\nX_train, y_train = Variable(X_train), Variable(y_train)\noutputs = model(X_train)\n_, pred = torch.max(outputs.data, 1)\n# l.append(pred)\nloss = cost(outputs, y_train)\nloss.backward()\noptimzer.step()\nrunning_loss += loss.item()\nrunning_correct += torch.sum(pred == y_train.data).cpu()\nval_correct = 0\nfor data2 in val_iter:\nmodel.eval()\nX_test, y_test = data2.text, (data2.label - 1)\nX_test, y_test = X_test.cuda(), y_test.cuda()\nX_test, y_test = Variable(X_test), Variable(y_test)\noutputs = model(X_test)\n_, pred = torch.max(outputs, 1)\nloss1 = cost(outputs, y_test)\nv_loss += loss1.item()\nval_correct += torch.sum(pred == y_test.data)\nrl = running_loss / len(train)\nvl = v_loss / len(train)\ntra = running_correct / len(train)\ntea = val_correct / len(val)\nlosslist.append(rl)\nacclist.append(tra)\nprint(\n\"Train Loss is :{:.4f},Train Accuracy is:{:.4f},Val Loss is :{:.4f},Val Accuracy is:{:.4f}\".format(rl, tra,\nvl, tea))\ntesting_correct = 0\nfor data3 in test_iter:\nmodel.eval()\nX_test, y_test = data3.text, (data3.label - 1)\nX_test, y_test = X_test.cuda(), y_test.cuda()\nX_test, y_test = Variable(X_test), Variable(y_test)\noutputs = model(X_test)\n_, pred = torch.max(outputs, 1)\ntesting_correct += torch.sum(pred == y_test.data)\nprint(\"Test Accuracy is:{:.4f}\".format(testing_correct / len(test)))\nreturn losslist, acclist\n\n\nmodel = RNN2()\nprint(model)\nloss1, acc1 = train1(model, 100, 0.01,0.005)\n", "timestamp": 1627279824, "type": "text", "updated_at": 1627284137, "url": "", "vote": {}}}